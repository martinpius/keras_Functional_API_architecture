{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alien-Revival.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPziOwRDvD68nyCEx8RgVFJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/keras_Functional_API_architecture/blob/main/Alien_Revival.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb6OSq06ekLx",
        "outputId": "ac0df6aa-8156-437c-ca5b-4edf5990e3af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import tensorflow as tf\n",
        "  print(f\"You are using Google colab with tensorflow version: {tf.__version__}\")\n",
        "except Exception as e:\n",
        "  COLAB = False\n",
        "  print(f\"{type(e)}: {e}\\n...Please Load Your Drive...\")\n",
        "\n",
        "def time_fmt(t):\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60)/60)\n",
        "  s = int(t % 60)\n",
        "  return f\"{h}: {m:>02}: {s:>05.2f}\""
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "You are using Google colab with tensorflow version: 2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AcxDmllif-dv",
        "outputId": "99f98bfd-eb8f-4e49-c2d9-b81aaf16d9e4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "time_fmt(123.4903)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0: 02: 03.00'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndGCumHOhdC4"
      },
      "source": [
        "#End to end autoencoder's for cifar10 dataset\n",
        "#We will train the autoencoder in two different way\n",
        "#As a ussual MLP and as a CNN"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdcvJQaRh4CS"
      },
      "source": [
        "#The ussual MLP "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIGKuhmfh7Wr"
      },
      "source": [
        "#Defining the sampling class (layer subclassing): We sample from the normal distribution to instantiate the decoder's inputs"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWTmjALyiMfO"
      },
      "source": [
        "class SampleGenerator(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    mu, sigma = inputs\n",
        "    dim1 = tf.shape(mu)[0]\n",
        "    dim2 = tf.shape(mu)[1]\n",
        "    eps = tf.keras.backend.random_normal(shape = (dim1, dim2))\n",
        "    return mu + tf.exp(0.5 * sigma) * eps\n",
        "  "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "360kfDm6i6sG"
      },
      "source": [
        "#Defining  the encoder's class (layer-subclassing) which encode the original data/image to some representation"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH_B4j0RjIPm"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, hidden = 64, intermediate = 128, name = 'encoder', **kwargs):\n",
        "    super(Encoder, self).__init__(name = name, **kwargs)\n",
        "    self.layer1 = tf.keras.layers.Dense(units = hidden, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.layer2 = tf.keras.layers.Dense(units = intermediate, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.mu = tf.keras.layers.Dense(units = intermediate, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.sigma = tf.keras.layers.Dense(units = intermediate, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.sample_generator = SampleGenerator()\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.layer1(inputs)\n",
        "    x = self.layer2(x)\n",
        "    mu = self.mu(x)\n",
        "    sigma = self.sigma(x)\n",
        "    z = self.sample_generator((mu, sigma))\n",
        "    return mu, sigma, z\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvuD8ebmnFUe"
      },
      "source": [
        "#Defining a decoder's block. This is a layer-subclassing procedure to recreate the original data/image"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onbRwBTHnSsa"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,original_dim, hidden = 64,intermediate = 128, name = 'decoder', **kwargs):\n",
        "    super(Decoder, self).__init__(name = name, **kwargs)\n",
        "    self.dense1 = tf.keras.layers.Dense(units = hidden, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.dense2 = tf.keras.layers.Dense(units = intermediate, activation = 'relu', kernel_initializer='random_normal')\n",
        "    self.out = tf.keras.layers.Dense(units = original_dim, activation ='sigmoid')\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    return self.out(x)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4AB6atkooEy"
      },
      "source": [
        "#The autoencoder's class: (this is a model-subclassing) procedure. Here we combine encoder's and the decoder's to get an end-to-end model"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiCyyquPo53g"
      },
      "source": [
        "class MyAutoEncoder(tf.keras.Model):\n",
        "  def __init__(self, original_dim, hidden = 64, intermediate = 128, name = 'vae', **kwargs):\n",
        "    super(MyAutoEncoder, self).__init__(name = name, **kwargs)\n",
        "    self.original_dim = original_dim\n",
        "    self.encoder = Encoder(hidden = hidden, intermediate = intermediate)\n",
        "    self.decoder = Decoder(original_dim = original_dim, hidden = hidden)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    mu, sigma, z = self.encoder(inputs)\n",
        "    re_build = self.decoder(z)\n",
        "    kl_Div = -0.5 * tf.reduce_mean(sigma - tf.square(mu) - tf.exp(sigma) + 1)\n",
        "    self.add_loss(kl_Div)\n",
        "    return re_build\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMrnv_5CrofW"
      },
      "source": [
        "#Instantiate our model "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GZUUUFmrtr3"
      },
      "source": [
        "original_dim = 784 #This is a flat version of  32,32,3 cifar10 image"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V--U0aVXr4Ap"
      },
      "source": [
        "VAE = MyAutoEncoder(original_dim,64,128)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWroepzxsJN9"
      },
      "source": [
        "#Get the data and pre-process (We only need a training set)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF4ECR-wsXDn",
        "outputId": "496e082f-b859-4be3-a8ae-6d4d895b3871"
      },
      "source": [
        "(x_train, _),(_, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZouHHsesm8i",
        "outputId": "2434be9d-7e9e-46cd-d179-2ac335ed925f"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfbzMB6SsrTV"
      },
      "source": [
        "x_train = x_train.reshape(60000,784).astype('float32')/255.0"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8mZUSZotsNV"
      },
      "source": [
        "#Change to tensorflow dataset for easy streaming\n",
        "x_train.shape\n",
        "train_dfm = tf.data.Dataset.from_tensor_slices(x_train)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oomRniPNt8tD"
      },
      "source": [
        "train_dfm = train_dfm.shuffle(buffer_size = 1024).batch(64)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpWxe9O8uKnz"
      },
      "source": [
        "epochs = 10"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMZgtYOcu_If"
      },
      "source": [
        "main_loss = tf.keras.losses.MeanAbsoluteError()\n",
        "metric_fn = tf.keras.metrics.Mean()\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate = 1e-3)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vbrGMRiuOpZ"
      },
      "source": [
        "#The training loop"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quPa4uBfuRMP",
        "outputId": "6b512717-3098-466f-c441-41b68719832e"
      },
      "source": [
        "tic = time.time()\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Here we begin training at epoch: {epoch}\")\n",
        "\n",
        "  for step, x_batch_train in enumerate(train_dfm):\n",
        "    with tf.GradientTape() as tape:\n",
        "      re_build = VAE(x_batch_train)\n",
        "      loss = main_loss(x_batch_train, re_build)\n",
        "      loss+=sum(VAE.losses)#Adding the KL_Divergence loss for autoencoder\n",
        "    grads = tape.gradient(loss, VAE.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, VAE.trainable_weights))\n",
        "    metric_fn(loss)\n",
        "    if step % 100 == 0:\n",
        "      print(\"step %d: mean loss = %.4f\" % (step, metric_fn.result()))\n",
        "toc = time.time()\n",
        "print(f\"Time elapsed: {time_fmt(tic - toc)}\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here we begin training at epoch: 0\n",
            "step 0: mean loss = 0.4745\n",
            "step 100: mean loss = 0.1680\n",
            "step 200: mean loss = 0.1469\n",
            "step 300: mean loss = 0.1395\n",
            "step 400: mean loss = 0.1362\n",
            "step 500: mean loss = 0.1337\n",
            "step 600: mean loss = 0.1323\n",
            "step 700: mean loss = 0.1311\n",
            "step 800: mean loss = 0.1303\n",
            "step 900: mean loss = 0.1295\n",
            "Here we begin training at epoch: 1\n",
            "step 0: mean loss = 0.1293\n",
            "step 100: mean loss = 0.1289\n",
            "step 200: mean loss = 0.1287\n",
            "step 300: mean loss = 0.1283\n",
            "step 400: mean loss = 0.1282\n",
            "step 500: mean loss = 0.1278\n",
            "step 600: mean loss = 0.1277\n",
            "step 700: mean loss = 0.1274\n",
            "step 800: mean loss = 0.1273\n",
            "step 900: mean loss = 0.1271\n",
            "Here we begin training at epoch: 2\n",
            "step 0: mean loss = 0.1270\n",
            "step 100: mean loss = 0.1269\n",
            "step 200: mean loss = 0.1269\n",
            "step 300: mean loss = 0.1267\n",
            "step 400: mean loss = 0.1267\n",
            "step 500: mean loss = 0.1266\n",
            "step 600: mean loss = 0.1265\n",
            "step 700: mean loss = 0.1264\n",
            "step 800: mean loss = 0.1264\n",
            "step 900: mean loss = 0.1262\n",
            "Here we begin training at epoch: 3\n",
            "step 0: mean loss = 0.1262\n",
            "step 100: mean loss = 0.1262\n",
            "step 200: mean loss = 0.1262\n",
            "step 300: mean loss = 0.1261\n",
            "step 400: mean loss = 0.1261\n",
            "step 500: mean loss = 0.1261\n",
            "step 600: mean loss = 0.1260\n",
            "step 700: mean loss = 0.1260\n",
            "step 800: mean loss = 0.1259\n",
            "step 900: mean loss = 0.1259\n",
            "Here we begin training at epoch: 4\n",
            "step 0: mean loss = 0.1258\n",
            "step 100: mean loss = 0.1258\n",
            "step 200: mean loss = 0.1258\n",
            "step 300: mean loss = 0.1258\n",
            "step 400: mean loss = 0.1258\n",
            "step 500: mean loss = 0.1258\n",
            "step 600: mean loss = 0.1257\n",
            "step 700: mean loss = 0.1257\n",
            "step 800: mean loss = 0.1257\n",
            "step 900: mean loss = 0.1256\n",
            "Here we begin training at epoch: 5\n",
            "step 0: mean loss = 0.1256\n",
            "step 100: mean loss = 0.1256\n",
            "step 200: mean loss = 0.1256\n",
            "step 300: mean loss = 0.1256\n",
            "step 400: mean loss = 0.1256\n",
            "step 500: mean loss = 0.1255\n",
            "step 600: mean loss = 0.1255\n",
            "step 700: mean loss = 0.1255\n",
            "step 800: mean loss = 0.1255\n",
            "step 900: mean loss = 0.1255\n",
            "Here we begin training at epoch: 6\n",
            "step 0: mean loss = 0.1254\n",
            "step 100: mean loss = 0.1254\n",
            "step 200: mean loss = 0.1255\n",
            "step 300: mean loss = 0.1254\n",
            "step 400: mean loss = 0.1254\n",
            "step 500: mean loss = 0.1254\n",
            "step 600: mean loss = 0.1254\n",
            "step 700: mean loss = 0.1254\n",
            "step 800: mean loss = 0.1254\n",
            "step 900: mean loss = 0.1253\n",
            "Here we begin training at epoch: 7\n",
            "step 0: mean loss = 0.1253\n",
            "step 100: mean loss = 0.1253\n",
            "step 200: mean loss = 0.1253\n",
            "step 300: mean loss = 0.1253\n",
            "step 400: mean loss = 0.1253\n",
            "step 500: mean loss = 0.1253\n",
            "step 600: mean loss = 0.1253\n",
            "step 700: mean loss = 0.1253\n",
            "step 800: mean loss = 0.1253\n",
            "step 900: mean loss = 0.1253\n",
            "Here we begin training at epoch: 8\n",
            "step 0: mean loss = 0.1252\n",
            "step 100: mean loss = 0.1252\n",
            "step 200: mean loss = 0.1253\n",
            "step 300: mean loss = 0.1252\n",
            "step 400: mean loss = 0.1253\n",
            "step 500: mean loss = 0.1252\n",
            "step 600: mean loss = 0.1252\n",
            "step 700: mean loss = 0.1252\n",
            "step 800: mean loss = 0.1252\n",
            "step 900: mean loss = 0.1252\n",
            "Here we begin training at epoch: 9\n",
            "step 0: mean loss = 0.1252\n",
            "step 100: mean loss = 0.1252\n",
            "step 200: mean loss = 0.1252\n",
            "step 300: mean loss = 0.1252\n",
            "step 400: mean loss = 0.1252\n",
            "step 500: mean loss = 0.1252\n",
            "step 600: mean loss = 0.1252\n",
            "step 700: mean loss = 0.1252\n",
            "step 800: mean loss = 0.1252\n",
            "step 900: mean loss = 0.1251\n",
            "Time elapsed: 0: 56: 39.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW9rbzB-wo-V"
      },
      "source": [
        "#Autoencoder's for cifar10 data"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow54BCoOI4J4"
      },
      "source": [
        "inputs = tf.keras.Input(shape = (32,32,3), name = 'cifar10_img')\n",
        "x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'valid')(inputs)\n",
        "x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu', padding = 'valid')(x)\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(x)\n",
        "x = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'valid')(x)\n",
        "x = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu', padding = 'valid')(x)\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "out = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Reshape((8,8,1))(out)\n",
        "x = tf.keras.layers.Conv2DTranspose(filters = 64, kernel_size = (3,3), activation = 'relu')(x)\n",
        "x = tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = (3,3), activation = 'relu')(x)\n",
        "x = tf.keras.layers.UpSampling2D(size = (2,2))(x)\n",
        "x = tf.keras.layers.Conv2DTranspose(filters = 64, kernel_size = (3,3), activation = 'relu')(x)\n",
        "x = tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = (3,3), activation = 'relu')(x)\n",
        "x = tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = (3,3), activation = 'relu')(x)\n",
        "og_img = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = (3,3), activation = 'relu', name = 'org_img')(x)\n",
        "vae = tf.keras.Model(inputs = inputs, outputs = og_img)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWo0zgtgNC86",
        "outputId": "e4afe084-d01e-4627-ae8f-87079c606f4e"
      },
      "source": [
        "vae.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cifar10_img (InputLayer)     [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 30, 30, 128)       3584      \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 10, 10, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d_8 (Glob (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "reshape_4 (Reshape)          (None, 8, 8, 1)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_20 (Conv2DT (None, 10, 10, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_21 (Conv2DT (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_22 (Conv2DT (None, 26, 26, 64)        73792     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_23 (Conv2DT (None, 28, 28, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_24 (Conv2DT (None, 30, 30, 128)       147584    \n",
            "_________________________________________________________________\n",
            "org_img (Conv2DTranspose)    (None, 32, 32, 3)         3459      \n",
            "=================================================================\n",
            "Total params: 598,467\n",
            "Trainable params: 598,339\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av_lSk9qPHcm"
      },
      "source": [
        "my_loss = tf.keras.losses.MeanAbsoluteError()\n",
        "my_metric = tf.keras.metrics.Mean()\n",
        "my_optimizer = tf.keras.optimizers.RMSprop(learning_rate = 1e-3)\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVe2fz4EQV9c"
      },
      "source": [
        "epochs = 15"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1lL9p0rQxBa"
      },
      "source": [
        "#Load cifar10 dataset\n",
        "(x_train, _),(_, _) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW42lQzoRB14"
      },
      "source": [
        "#Preprocess the data and convert to tensorflow datatype in batches of size 64 each\n",
        "x_train = x_train.astype('float32')/255.0"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi1QQR15Rbxc",
        "outputId": "c2d099e2-1dc8-4e81-e800-7af25f3aa45a"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq5_9JkLRf95"
      },
      "source": [
        "train_dfm = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dfm = train_dfm.shuffle(buffer_size = 1024).batch(64)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXWxNBgTQZoO",
        "outputId": "e0de4101-5b0b-4389-9852-a27ea80354ad"
      },
      "source": [
        "tic = time.time()\n",
        "for epoch in range(epochs):\n",
        "  print(f\"The start of epoch: {epoch}\")\n",
        "  for step,x_train_batch in enumerate(train_dfm):\n",
        "    with tf.GradientTape() as tape:\n",
        "      org_img = vae(x_train_batch)\n",
        "      loss = my_loss(x_train_batch, org_img)\n",
        "      loss+= loss\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    my_optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "    my_metric(loss)\n",
        "    if step % 100 == 0:\n",
        "      print(\"step %d: mean loss = %.4f\" % (step, my_metric.result()))\n",
        "toc = time.time()\n",
        "print(f\"time elapsed: {tic - toc}\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The start of epoch: 0\n",
            "step 0: mean loss = 0.9166\n",
            "step 100: mean loss = 0.5413\n",
            "step 200: mean loss = 0.4840\n",
            "step 300: mean loss = 0.4627\n",
            "step 400: mean loss = 0.4515\n",
            "step 500: mean loss = 0.4424\n",
            "step 600: mean loss = 0.4344\n",
            "step 700: mean loss = 0.4257\n",
            "The start of epoch: 1\n",
            "step 0: mean loss = 0.4190\n",
            "step 100: mean loss = 0.4120\n",
            "step 200: mean loss = 0.4059\n",
            "step 300: mean loss = 0.4003\n",
            "step 400: mean loss = 0.3949\n",
            "step 500: mean loss = 0.3904\n",
            "step 600: mean loss = 0.3860\n",
            "step 700: mean loss = 0.3821\n",
            "The start of epoch: 2\n",
            "step 0: mean loss = 0.3789\n",
            "step 100: mean loss = 0.3755\n",
            "step 200: mean loss = 0.3723\n",
            "step 300: mean loss = 0.3694\n",
            "step 400: mean loss = 0.3666\n",
            "step 500: mean loss = 0.3641\n",
            "step 600: mean loss = 0.3616\n",
            "step 700: mean loss = 0.3593\n",
            "The start of epoch: 3\n",
            "step 0: mean loss = 0.3574\n",
            "step 100: mean loss = 0.3552\n",
            "step 200: mean loss = 0.3531\n",
            "step 300: mean loss = 0.3513\n",
            "step 400: mean loss = 0.3494\n",
            "step 500: mean loss = 0.3477\n",
            "step 600: mean loss = 0.3459\n",
            "step 700: mean loss = 0.3443\n",
            "The start of epoch: 4\n",
            "step 0: mean loss = 0.3429\n",
            "step 100: mean loss = 0.3413\n",
            "step 200: mean loss = 0.3399\n",
            "step 300: mean loss = 0.3385\n",
            "step 400: mean loss = 0.3371\n",
            "step 500: mean loss = 0.3358\n",
            "step 600: mean loss = 0.3345\n",
            "step 700: mean loss = 0.3332\n",
            "The start of epoch: 5\n",
            "step 0: mean loss = 0.3322\n",
            "step 100: mean loss = 0.3310\n",
            "step 200: mean loss = 0.3298\n",
            "step 300: mean loss = 0.3288\n",
            "step 400: mean loss = 0.3277\n",
            "step 500: mean loss = 0.3266\n",
            "step 600: mean loss = 0.3256\n",
            "step 700: mean loss = 0.3246\n",
            "The start of epoch: 6\n",
            "step 0: mean loss = 0.3238\n",
            "step 100: mean loss = 0.3228\n",
            "step 200: mean loss = 0.3218\n",
            "step 300: mean loss = 0.3210\n",
            "step 400: mean loss = 0.3201\n",
            "step 500: mean loss = 0.3192\n",
            "step 600: mean loss = 0.3184\n",
            "step 700: mean loss = 0.3176\n",
            "The start of epoch: 7\n",
            "step 0: mean loss = 0.3169\n",
            "step 100: mean loss = 0.3161\n",
            "step 200: mean loss = 0.3153\n",
            "step 300: mean loss = 0.3145\n",
            "step 400: mean loss = 0.3138\n",
            "step 500: mean loss = 0.3131\n",
            "step 600: mean loss = 0.3123\n",
            "step 700: mean loss = 0.3116\n",
            "The start of epoch: 8\n",
            "step 0: mean loss = 0.3110\n",
            "step 100: mean loss = 0.3104\n",
            "step 200: mean loss = 0.3097\n",
            "step 300: mean loss = 0.3091\n",
            "step 400: mean loss = 0.3084\n",
            "step 500: mean loss = 0.3078\n",
            "step 600: mean loss = 0.3072\n",
            "step 700: mean loss = 0.3066\n",
            "The start of epoch: 9\n",
            "step 0: mean loss = 0.3060\n",
            "step 100: mean loss = 0.3055\n",
            "step 200: mean loss = 0.3049\n",
            "step 300: mean loss = 0.3043\n",
            "step 400: mean loss = 0.3038\n",
            "step 500: mean loss = 0.3032\n",
            "step 600: mean loss = 0.3027\n",
            "step 700: mean loss = 0.3022\n",
            "The start of epoch: 10\n",
            "step 0: mean loss = 0.3017\n",
            "step 100: mean loss = 0.3012\n",
            "step 200: mean loss = 0.3007\n",
            "step 300: mean loss = 0.3002\n",
            "step 400: mean loss = 0.2997\n",
            "step 500: mean loss = 0.2992\n",
            "step 600: mean loss = 0.2987\n",
            "step 700: mean loss = 0.2983\n",
            "The start of epoch: 11\n",
            "step 0: mean loss = 0.2979\n",
            "step 100: mean loss = 0.2974\n",
            "step 200: mean loss = 0.2970\n",
            "step 300: mean loss = 0.2965\n",
            "step 400: mean loss = 0.2961\n",
            "step 500: mean loss = 0.2957\n",
            "step 600: mean loss = 0.2952\n",
            "step 700: mean loss = 0.2948\n",
            "The start of epoch: 12\n",
            "step 0: mean loss = 0.2945\n",
            "step 100: mean loss = 0.2940\n",
            "step 200: mean loss = 0.2936\n",
            "step 300: mean loss = 0.2933\n",
            "step 400: mean loss = 0.2929\n",
            "step 500: mean loss = 0.2925\n",
            "step 600: mean loss = 0.2921\n",
            "step 700: mean loss = 0.2917\n",
            "The start of epoch: 13\n",
            "step 0: mean loss = 0.2914\n",
            "step 100: mean loss = 0.2910\n",
            "step 200: mean loss = 0.2907\n",
            "step 300: mean loss = 0.2903\n",
            "step 400: mean loss = 0.2900\n",
            "step 500: mean loss = 0.2896\n",
            "step 600: mean loss = 0.2893\n",
            "step 700: mean loss = 0.2889\n",
            "The start of epoch: 14\n",
            "step 0: mean loss = 0.2886\n",
            "step 100: mean loss = 0.2883\n",
            "step 200: mean loss = 0.2880\n",
            "step 300: mean loss = 0.2877\n",
            "step 400: mean loss = 0.2873\n",
            "step 500: mean loss = 0.2870\n",
            "step 600: mean loss = 0.2867\n",
            "step 700: mean loss = 0.2864\n",
            "time elapsed: -452.43489956855774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slg-cBsLbgVt",
        "outputId": "dd553292-14e9-4fb0-cbf1-2168d2441b06"
      },
      "source": [
        "print(f\"time elapsed: {time_fmt(toc - tic)}\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time elapsed: 0: 07: 32.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AwgMbq6T_s4"
      },
      "source": [
        ""
      ],
      "execution_count": 62,
      "outputs": []
    }
  ]
}