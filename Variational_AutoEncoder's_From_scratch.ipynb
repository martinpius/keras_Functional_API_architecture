{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Variational AutoEncoder's From scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/tZLnfk3VgPOhbzBd7yxa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/keras_Functional_API_architecture/blob/main/Variational_AutoEncoder's_From_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aBG1mHCMpNm",
        "outputId": "662d2522-5079-4da9-f9b9-85cccf1e5feb"
      },
      "source": [
        "from google.colab import drive\n",
        "try:\n",
        "  drive.mount('/content/drive/', force_remount = True)\n",
        "  COLAB = True\n",
        "  import tensorflow as tf\n",
        "  print(f\"You are using google colab with tensoflow version: {tf.__version__}\")\n",
        "except Exception as e:\n",
        "  COLAB = False\n",
        "  print(f\"{type(e)}: {e}\\n...load your drive...\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "You are using google colab with tensoflow version: 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITRgsWOMTaHz"
      },
      "source": [
        "#We are going to train the mnist dataset on a Variational autoencoder model\n",
        "#This model simply try to represent the data in a more compact form such that\n",
        "#through sampling we can reconstruct back the original data\n",
        "#Example we can take input as an image and convolves to obtain the pixels (decoding)\n",
        "#Then from the pixels we can sample (assuming Gaussian distribution to reconstruct the original image)\n",
        "#The autoencoder model is trainee in three few steps\n",
        "#First we build an encoder model (to encode the original information)\n",
        "#Second we build a decoder network to reconstruct the original information\n",
        "#Finally we combine the two netwoks to train it frromnehnd to end\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBwZRfV0dcBC"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6NUmum9dJcy"
      },
      "source": [
        "#The sampling mechanism (layer's subclassing)\n",
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    '''This method will return the inputs to a decoder network'''\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    eps = tf.keras.backend.random_normal(shape = (batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7GjE5fqeh68"
      },
      "source": [
        "#The encoder's network (layer's subclassing)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlV7z5AVelSU"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, latent = 32, intermediate = 64, name = 'encoder', **kwargs):\n",
        "    super(Encoder, self).__init__(name = name, **kwargs)\n",
        "    self.dense_1 = tf.keras.layers.Dense(units = intermediate, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.z_mean = tf.keras.layers.Dense(units = latent, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.z_log_var = tf.keras.layers.Dense(units = latent, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.sampling = Sampling()\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.dense_1(inputs)\n",
        "    z_mean = self.z_mean(x)\n",
        "    z_log_var = self.z_log_var(x)\n",
        "    z = self.sampling((z_mean, z_log_var))\n",
        "    return z_mean, z_log_var, z\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc-kSxkfgWHw"
      },
      "source": [
        "#The decoder's network (layer's subclassing)\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, original, intermediate = 64, name = 'decoder', **kwargs):\n",
        "    super(Decoder, self).__init__(name = name, **kwargs)\n",
        "    self.dense_1 = tf.keras.layers.Dense(units = intermediate, activation = 'relu', kernel_initializer = 'random_normal')\n",
        "    self.out_decoder = tf.keras.layers.Dense(units = original, activation = 'sigmoid')\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.dense_1(inputs)\n",
        "    return self.out_decoder(x)\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L2zCk4vh6iX"
      },
      "source": [
        "#Combining both of the above\n",
        "class VariationalAutoEncoder(tf.keras.Model):\n",
        "  def __init__(self, original, latent = 32, intermediate = 64, name = 'vae',**kwargs):\n",
        "    super(VariationalAutoEncoder, self).__init__(name = name, **kwargs)\n",
        "    self.original = original\n",
        "    self.encoder = Encoder(latent = latent, intermediate = intermediate)\n",
        "    self.decoder = Decoder(original = original, intermediate = intermediate)\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var, z = self.encoder(inputs)\n",
        "    reconstructed = self.decoder(z)\n",
        "    kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "    self.add_loss(kl_loss)\n",
        "    return reconstructed"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w45WGYR3kEZy"
      },
      "source": [
        "original = 784\n",
        "vae = VariationalAutoEncoder(original, 32, 64)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFT13wGMkTEZ",
        "outputId": "85901ad7-1870-49f6-a880-c4dad036f67d"
      },
      "source": [
        "#Get the train data from keras-mnist digits\n",
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "dtUobF64k9RI",
        "outputId": "fa02baf7-37cd-40c8-ad60-991670f9e104"
      },
      "source": [
        "display(x_train.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-TTEgQdkky-"
      },
      "source": [
        "#reshaping,preprocessing, and converting into a tensorflow dataset\n",
        "x_train = x_train.reshape(60000, 784).astype('float32')/255.0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLSb3NoDlM6k"
      },
      "source": [
        "train_dfm = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dfm = train_dfm.shuffle(buffer_size = 1024).batch(64)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-firNcflqs7"
      },
      "source": [
        "#We can now preparing the training loop for our autoencoder\n",
        "epochs = 10\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "metric_fn = tf.keras.metrics.Mean()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Uqlcya9mOri",
        "outputId": "edea8f0a-4256-46c5-b10e-f7b9ec26d6b1"
      },
      "source": [
        "#Iterate over the epochs\n",
        "for epoch in range(epochs):\n",
        "  print(f\"The start of epoch {epoch}.....please wait....\")\n",
        "  #iterate over the training batches\n",
        "  for step, train_batch in enumerate(train_dfm):\n",
        "    with tf.GradientTape() as tape:\n",
        "      reconstructed = vae(train_batch)\n",
        "      loss = loss_fn(train_batch, reconstructed)\n",
        "      loss+=sum(vae.losses) #Combining the kl-loss to the main loss.(Note that we can train the model with either of the loss)\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "    metric_fn(loss)\n",
        "    if step % 50 ==0:\n",
        "      print(f\"Step: {step} mean-loss: {metric_fn.result():.4f}\")\n",
        "  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The start of epoch 0.....please wait....\n",
            "Step: 0 mean-loss: 0.2374\n",
            "Step: 50 mean-loss: 0.1872\n",
            "Step: 100 mean-loss: 0.1450\n",
            "Step: 150 mean-loss: 0.1221\n",
            "Step: 200 mean-loss: 0.1096\n",
            "Step: 250 mean-loss: 0.1017\n",
            "Step: 300 mean-loss: 0.0961\n",
            "Step: 350 mean-loss: 0.0924\n",
            "Step: 400 mean-loss: 0.0894\n",
            "Step: 450 mean-loss: 0.0871\n",
            "Step: 500 mean-loss: 0.0850\n",
            "Step: 550 mean-loss: 0.0834\n",
            "Step: 600 mean-loss: 0.0822\n",
            "Step: 650 mean-loss: 0.0811\n",
            "Step: 700 mean-loss: 0.0800\n",
            "Step: 750 mean-loss: 0.0792\n",
            "Step: 800 mean-loss: 0.0785\n",
            "Step: 850 mean-loss: 0.0778\n",
            "Step: 900 mean-loss: 0.0772\n",
            "The start of epoch 1.....please wait....\n",
            "Step: 0 mean-loss: 0.0768\n",
            "Step: 50 mean-loss: 0.0763\n",
            "Step: 100 mean-loss: 0.0759\n",
            "Step: 150 mean-loss: 0.0756\n",
            "Step: 200 mean-loss: 0.0753\n",
            "Step: 250 mean-loss: 0.0750\n",
            "Step: 300 mean-loss: 0.0746\n",
            "Step: 350 mean-loss: 0.0744\n",
            "Step: 400 mean-loss: 0.0742\n",
            "Step: 450 mean-loss: 0.0740\n",
            "Step: 500 mean-loss: 0.0737\n",
            "Step: 550 mean-loss: 0.0734\n",
            "Step: 600 mean-loss: 0.0733\n",
            "Step: 650 mean-loss: 0.0731\n",
            "Step: 700 mean-loss: 0.0729\n",
            "Step: 750 mean-loss: 0.0728\n",
            "Step: 800 mean-loss: 0.0726\n",
            "Step: 850 mean-loss: 0.0724\n",
            "Step: 900 mean-loss: 0.0723\n",
            "The start of epoch 2.....please wait....\n",
            "Step: 0 mean-loss: 0.0722\n",
            "Step: 50 mean-loss: 0.0720\n",
            "Step: 100 mean-loss: 0.0719\n",
            "Step: 150 mean-loss: 0.0718\n",
            "Step: 200 mean-loss: 0.0718\n",
            "Step: 250 mean-loss: 0.0717\n",
            "Step: 300 mean-loss: 0.0716\n",
            "Step: 350 mean-loss: 0.0715\n",
            "Step: 400 mean-loss: 0.0714\n",
            "Step: 450 mean-loss: 0.0713\n",
            "Step: 500 mean-loss: 0.0712\n",
            "Step: 550 mean-loss: 0.0711\n",
            "Step: 600 mean-loss: 0.0711\n",
            "Step: 650 mean-loss: 0.0710\n",
            "Step: 700 mean-loss: 0.0709\n",
            "Step: 750 mean-loss: 0.0709\n",
            "Step: 800 mean-loss: 0.0708\n",
            "Step: 850 mean-loss: 0.0707\n",
            "Step: 900 mean-loss: 0.0706\n",
            "The start of epoch 3.....please wait....\n",
            "Step: 0 mean-loss: 0.0706\n",
            "Step: 50 mean-loss: 0.0705\n",
            "Step: 100 mean-loss: 0.0705\n",
            "Step: 150 mean-loss: 0.0704\n",
            "Step: 200 mean-loss: 0.0704\n",
            "Step: 250 mean-loss: 0.0704\n",
            "Step: 300 mean-loss: 0.0703\n",
            "Step: 350 mean-loss: 0.0703\n",
            "Step: 400 mean-loss: 0.0703\n",
            "Step: 450 mean-loss: 0.0702\n",
            "Step: 500 mean-loss: 0.0702\n",
            "Step: 550 mean-loss: 0.0701\n",
            "Step: 600 mean-loss: 0.0701\n",
            "Step: 650 mean-loss: 0.0700\n",
            "Step: 700 mean-loss: 0.0700\n",
            "Step: 750 mean-loss: 0.0700\n",
            "Step: 800 mean-loss: 0.0699\n",
            "Step: 850 mean-loss: 0.0699\n",
            "Step: 900 mean-loss: 0.0698\n",
            "The start of epoch 4.....please wait....\n",
            "Step: 0 mean-loss: 0.0698\n",
            "Step: 50 mean-loss: 0.0698\n",
            "Step: 100 mean-loss: 0.0697\n",
            "Step: 150 mean-loss: 0.0697\n",
            "Step: 200 mean-loss: 0.0697\n",
            "Step: 250 mean-loss: 0.0697\n",
            "Step: 300 mean-loss: 0.0696\n",
            "Step: 350 mean-loss: 0.0696\n",
            "Step: 400 mean-loss: 0.0696\n",
            "Step: 450 mean-loss: 0.0696\n",
            "Step: 500 mean-loss: 0.0696\n",
            "Step: 550 mean-loss: 0.0695\n",
            "Step: 600 mean-loss: 0.0695\n",
            "Step: 650 mean-loss: 0.0695\n",
            "Step: 700 mean-loss: 0.0694\n",
            "Step: 750 mean-loss: 0.0694\n",
            "Step: 800 mean-loss: 0.0694\n",
            "Step: 850 mean-loss: 0.0694\n",
            "Step: 900 mean-loss: 0.0693\n",
            "The start of epoch 5.....please wait....\n",
            "Step: 0 mean-loss: 0.0693\n",
            "Step: 50 mean-loss: 0.0693\n",
            "Step: 100 mean-loss: 0.0693\n",
            "Step: 150 mean-loss: 0.0693\n",
            "Step: 200 mean-loss: 0.0693\n",
            "Step: 250 mean-loss: 0.0692\n",
            "Step: 300 mean-loss: 0.0692\n",
            "Step: 350 mean-loss: 0.0692\n",
            "Step: 400 mean-loss: 0.0692\n",
            "Step: 450 mean-loss: 0.0692\n",
            "Step: 500 mean-loss: 0.0692\n",
            "Step: 550 mean-loss: 0.0691\n",
            "Step: 600 mean-loss: 0.0691\n",
            "Step: 650 mean-loss: 0.0691\n",
            "Step: 700 mean-loss: 0.0691\n",
            "Step: 750 mean-loss: 0.0691\n",
            "Step: 800 mean-loss: 0.0691\n",
            "Step: 850 mean-loss: 0.0690\n",
            "Step: 900 mean-loss: 0.0690\n",
            "The start of epoch 6.....please wait....\n",
            "Step: 0 mean-loss: 0.0690\n",
            "Step: 50 mean-loss: 0.0690\n",
            "Step: 100 mean-loss: 0.0690\n",
            "Step: 150 mean-loss: 0.0690\n",
            "Step: 200 mean-loss: 0.0690\n",
            "Step: 250 mean-loss: 0.0689\n",
            "Step: 300 mean-loss: 0.0689\n",
            "Step: 350 mean-loss: 0.0689\n",
            "Step: 400 mean-loss: 0.0689\n",
            "Step: 450 mean-loss: 0.0689\n",
            "Step: 500 mean-loss: 0.0689\n",
            "Step: 550 mean-loss: 0.0689\n",
            "Step: 600 mean-loss: 0.0689\n",
            "Step: 650 mean-loss: 0.0688\n",
            "Step: 700 mean-loss: 0.0688\n",
            "Step: 750 mean-loss: 0.0688\n",
            "Step: 800 mean-loss: 0.0688\n",
            "Step: 850 mean-loss: 0.0688\n",
            "Step: 900 mean-loss: 0.0688\n",
            "The start of epoch 7.....please wait....\n",
            "Step: 0 mean-loss: 0.0688\n",
            "Step: 50 mean-loss: 0.0687\n",
            "Step: 100 mean-loss: 0.0687\n",
            "Step: 150 mean-loss: 0.0687\n",
            "Step: 200 mean-loss: 0.0687\n",
            "Step: 250 mean-loss: 0.0687\n",
            "Step: 300 mean-loss: 0.0687\n",
            "Step: 350 mean-loss: 0.0687\n",
            "Step: 400 mean-loss: 0.0687\n",
            "Step: 450 mean-loss: 0.0687\n",
            "Step: 500 mean-loss: 0.0687\n",
            "Step: 550 mean-loss: 0.0687\n",
            "Step: 600 mean-loss: 0.0687\n",
            "Step: 650 mean-loss: 0.0686\n",
            "Step: 700 mean-loss: 0.0686\n",
            "Step: 750 mean-loss: 0.0686\n",
            "Step: 800 mean-loss: 0.0686\n",
            "Step: 850 mean-loss: 0.0686\n",
            "Step: 900 mean-loss: 0.0686\n",
            "The start of epoch 8.....please wait....\n",
            "Step: 0 mean-loss: 0.0686\n",
            "Step: 50 mean-loss: 0.0686\n",
            "Step: 100 mean-loss: 0.0686\n",
            "Step: 150 mean-loss: 0.0686\n",
            "Step: 200 mean-loss: 0.0686\n",
            "Step: 250 mean-loss: 0.0686\n",
            "Step: 300 mean-loss: 0.0685\n",
            "Step: 350 mean-loss: 0.0685\n",
            "Step: 400 mean-loss: 0.0685\n",
            "Step: 450 mean-loss: 0.0685\n",
            "Step: 500 mean-loss: 0.0685\n",
            "Step: 550 mean-loss: 0.0685\n",
            "Step: 600 mean-loss: 0.0685\n",
            "Step: 650 mean-loss: 0.0685\n",
            "Step: 700 mean-loss: 0.0685\n",
            "Step: 750 mean-loss: 0.0685\n",
            "Step: 800 mean-loss: 0.0685\n",
            "Step: 850 mean-loss: 0.0685\n",
            "Step: 900 mean-loss: 0.0684\n",
            "The start of epoch 9.....please wait....\n",
            "Step: 0 mean-loss: 0.0684\n",
            "Step: 50 mean-loss: 0.0684\n",
            "Step: 100 mean-loss: 0.0684\n",
            "Step: 150 mean-loss: 0.0684\n",
            "Step: 200 mean-loss: 0.0684\n",
            "Step: 250 mean-loss: 0.0684\n",
            "Step: 300 mean-loss: 0.0684\n",
            "Step: 350 mean-loss: 0.0684\n",
            "Step: 400 mean-loss: 0.0684\n",
            "Step: 450 mean-loss: 0.0684\n",
            "Step: 500 mean-loss: 0.0684\n",
            "Step: 550 mean-loss: 0.0684\n",
            "Step: 600 mean-loss: 0.0684\n",
            "Step: 650 mean-loss: 0.0684\n",
            "Step: 700 mean-loss: 0.0684\n",
            "Step: 750 mean-loss: 0.0684\n",
            "Step: 800 mean-loss: 0.0684\n",
            "Step: 850 mean-loss: 0.0683\n",
            "Step: 900 mean-loss: 0.0683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtQydoepohsy"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}